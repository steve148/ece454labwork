Q1. Why is it important to #ifdef out methods and datastructures that arenâ€™t used for different versions of randtrack?

In the case of using one single file, #ifdef is important 

Q2. How difficult was using TM compared to implementing global lock above?

TM was stupid easy.

Q3. Can you implement this without modifying the hash class, or without knowing its internal implementation?

No since things like how the key gets hashed / translated into the list index are necessary.

Q4. Can you properly implement this solely by modifying the hash class methods lookup and insert? Explain.

The insert function takes in a sample object so it can't have 

Q5. Can you implement this by adding to the hash class a new function lookup and insert if absent? Explain.

No, the reason being that since the count is modified outside the hash class then the lock inside the hash function lookup_and_insert_if_absent would not guarentee synchronization on the count. 

Q6. Can you implement it by adding new methods to the hash class lock list and unlock list? Explain.
Implement the simplest solution above that works (or a better one if you can think of one).

Yes, this is the solution we implemented. This provides a more fine grained control of when the hash list gets locked but the hazard of the caller being responsible for locking / unlocking the lists.

Q7. How difficult was using TM compared to implementing list locking above?

TM impleneted here was even easier.

Q8. What are the pros and cons of this approach?

Sample function doesn't have any locking because of seperate hash tables and therefore makes coding the sample function easier. Con of approach is that it adds overhead of having to reduce the hash tables into one, which if the hash tables were larger / of a non optimal size then it could increase the program runtime by a significant amount.

Q9. For samples to skip set to 50, what is the overhead for each parallelization approach? Report this as the runtime of the parallel version with one thread divided by the runtime of the single-threaded version.



Q10. How does each approach perform as the number of threads increases? If performance gets worse for a certain case, explain why that may have happened.

Only case where performance got worst was for TM running on a single thread, aka. no multithreading. It went from ~10seconds runtime to ~11.5seconds runtime. This could be because the overhead caused by transactional memory is significant enough in the case of a single thread it is noticeably slower.

Q11. Repeat the data collection above with samples to skip set to 100 and give the table. How does this change impact the results compared with when set to 50? Why?



Q12. Which approach should OptsRus ship? Keep in mind that some customers might be using multicores with more than 4 cores, while others might have only one or two cores.



